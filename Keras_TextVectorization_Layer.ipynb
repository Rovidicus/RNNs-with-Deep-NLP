{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7104669d-237c-4f41-822e-967d4ebe6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Then Set Random Seeds\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "# Then run the Enable Deterministic Operations Function\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be89f706-db89-41fd-9cbf-239f4b484d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n",
    "pd.set_option('display.max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dbb7936-dee2-4a7b-8292-755804352425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Python's simplicity makes it a go-to language for rapid development.</td>\n",
       "      <td>programming/python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science is like a treasure hunt, and Python is my map!</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python frameworks like Django make web development a piece of cake.</td>\n",
       "      <td>programming/python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python's ecosystem is rich with libraries for both Data Science and Web Development.</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>With NLP, sentiment analysis is easier and more accurate.</td>\n",
       "      <td>data science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   text  \\\n",
       "0                  Python's simplicity makes it a go-to language for rapid development.   \n",
       "1                           Data Science is like a treasure hunt, and Python is my map!   \n",
       "2                   Python frameworks like Django make web development a piece of cake.   \n",
       "3  Python's ecosystem is rich with libraries for both Data Science and Web Development.   \n",
       "4                             With NLP, sentiment analysis is easier and more accurate.   \n",
       "\n",
       "                label  \n",
       "0  programming/python  \n",
       "1                both  \n",
       "2  programming/python  \n",
       "3                both  \n",
       "4        data science  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load from your file path\n",
    "df_demo = pd.read_csv(\"Data/programming-or-data-science.csv\")\n",
    "df_demo.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b03a959c-c459-4c04-b06f-53518967e92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    31\n",
       "2    13\n",
       "0     6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define X and y\n",
    "X = df_demo['text']\n",
    "y_string= df_demo['label']\n",
    "# Instantiate the LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "# Fit and Transform the strings into integers\n",
    "y = pd.Series(encoder.fit_transform(y_string))\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09499ca2-a889-4973-b492-3bf9512210e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to Dataset object\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1a6098-c2ba-46ea-8c0b-7d17334bc752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the data once\n",
    "ds = ds.shuffle(buffer_size=len(ds), reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6c67d53-9ce5-4391-b058-1881798b61d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 35 samples as training data\n",
      "Use 10 samples as validation data\n",
      "The remaining 5 samples will be used as test data.\n"
     ]
    }
   ],
   "source": [
    "# Determing how many samples for each split\n",
    "# Calculate the number of samples for training \n",
    "split_train = 0.7\n",
    "n_train_samples =  int(len(ds) * split_train)\n",
    "print(f\"Use {n_train_samples} samples as training data\")\n",
    "# Calculate the number of samples for validation\n",
    "split_val = 0.2\n",
    "n_val_samples = int(len(ds) * split_val)\n",
    "print(f\"Use {n_val_samples} samples as validation data\")\n",
    "# Test size is remainder\n",
    "split_test = 1 - (split_train + split_val)\n",
    "print(f\"The remaining {len(ds)- (n_train_samples+n_val_samples)} samples will be used as test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66ba218f-6d4d-4edf-982e-17251345c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .take to slice out the number of samples for training\n",
    "train_ds = ds.take(n_train_samples)\n",
    "# Skipover the training batches\n",
    "val_ds = ds.skip(n_train_samples)\n",
    "# Take .take to slice out the correct number of samples for validation\n",
    "val_ds = val_ds.take(n_val_samples)\n",
    "# Skip over all of the training + validation samples, the rest remain as samples for testing\n",
    "test_ds = ds.skip(n_train_samples + n_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86955991-5e7b-4952-892e-e93ffe9961c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling just the training data\n",
    "train_ds  = train_ds.shuffle(buffer_size = len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "298e346b-9672-48aa-9ab0-d72a31477b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are 35 training batches.\n",
      " There are 10 validation batches.\n",
      " There are 5 testing batches.\n"
     ]
    }
   ],
   "source": [
    "#  Setting the batch_size for all datasets\n",
    "BATCH_SIZE = 1\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "val_ds = val_ds.batch(BATCH_SIZE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)\n",
    "# Confirm the number of batches in each\n",
    "print (f' There are {len(train_ds)} training batches.')\n",
    "print (f' There are {len(val_ds)} validation batches.')\n",
    "print (f' There are {len(test_ds)} testing batches.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6db5da9b-bcc2-4333-9a45-7a337cb6083a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'I love programming, I would give it an A+!'], shape=(1,), dtype=string)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# taking a sample\n",
    "example_X, example_y= train_ds.take(1).get_single_element()\n",
    "print(example_X)\n",
    "print(example_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10bec1dc-3de1-4b28-bd3a-a72a3750506f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=\n",
       "array([b'The power of Data Science lies in turning raw data into meaningful insights.'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get just the text from ds_train\n",
    "ds_texts = train_ds.map(lambda x, y: x)\n",
    "# Preview the text\n",
    "ds_texts.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "098c1170-7c35-4ec9-a2cb-e0a9aabf1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TextVectorization layer\n",
    "count_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad4ece31-b4c8-4adc-868b-b482e2ff419b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before training, only contains the out of vocab token ([UNK])\n",
    "count_vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf10185a-ea63-4440-8f1f-616542ce30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the layer on the training texts\n",
    "count_vectorizer.adapt(ds_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5ba1667-a4fe-465c-974c-9af81f85d1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 180, ['[UNK]', 'is', 'data', 'the', 'science', 'python'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of vocab\n",
    "vocab = count_vectorizer.get_vocabulary()\n",
    "# Exploring list of vocab\n",
    "type(vocab), len(vocab), vocab[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4492c9c-b23a-4f05-9564-0146c2f4c279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 180), dtype=float32, numpy=\n",
       "array([[3., 1., 1., 2., 1., 4., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first value will be the count of all of the words not in the vocobulary\n",
    "counts= count_vectorizer(['python python python python is the most amazing thing in the world for data science!'])\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6249659-20cf-4e69-9642-843c533cffe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Text Vectorization Layer\n",
    "tfidf_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    output_mode=\"tf_idf\",\n",
    ")\n",
    "# Build the vectorizer vocabulary\n",
    "tfidf_vectorizer.adapt(ds_texts)\n",
    "# Confrim vocabulary size\n",
    "tfidf_vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0a40b62-4ede-43f1-8b46-9915f99a0deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 180), dtype=float32, numpy=\n",
       "array([[8.083353  , 0.95200884, 1.0799202 , 2.3184738 , 1.2039728 ,\n",
       "        5.2250066 , 1.3062516 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.5869651 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 2.9177706 , 0.        , 0.        ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first value will be the score of all of the words not in the vocobulary\n",
    "tfidf= tfidf_vectorizer(['python python python python is the most amazing thing in the world for data science!'])\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b93121c-48bd-4bd1-a907-911ca28a8a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create text Vectorization layer\n",
    "sequence_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=30\n",
    ")\n",
    "sequence_vectorizer.adapt(ds_texts)\n",
    "sequence_vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1751622-dc9b-4d94-b534-9073dab956d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 30), dtype=int64, numpy=\n",
       "array([[  6,   6,   6,   6,   2,   4,   1, 178,   1,   7,   4,   1,  13,\n",
       "          3,   5,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the sequence of sample text with the sequence_vectorizer\n",
    "sequence = sequence_vectorizer(['python python python python is the most amazing thing in the world for data science!'])\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b05ba535-1e09-4b8d-aae5-d6ab4156e786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '',\n",
       " 1: '[UNK]',\n",
       " 2: 'is',\n",
       " 3: 'data',\n",
       " 4: 'the',\n",
       " 5: 'science',\n",
       " 6: 'python',\n",
       " 7: 'in',\n",
       " 8: 'of',\n",
       " 9: 'a',\n",
       " 10: 'to',\n",
       " 11: 'language',\n",
       " 12: 'and',\n",
       " 13: 'for',\n",
       " 14: 'like',\n",
       " 15: 'programming',\n",
       " 16: 'natural',\n",
       " 17: 'development',\n",
       " 18: 'processing',\n",
       " 19: 'nlp',\n",
       " 20: 'web',\n",
       " 21: 'it',\n",
       " 22: 'field',\n",
       " 23: 'can',\n",
       " 24: 'analysis',\n",
       " 25: 'with',\n",
       " 26: 'used',\n",
       " 27: 'my',\n",
       " 28: 'making',\n",
       " 29: 'libraries',\n",
       " 30: 'its',\n",
       " 31: 'from',\n",
       " 32: 'also',\n",
       " 33: 'two',\n",
       " 34: 'text',\n",
       " 35: 'pythons',\n",
       " 36: 'pandas',\n",
       " 37: 'more',\n",
       " 38: 'make',\n",
       " 39: 'machines',\n",
       " 40: 'love',\n",
       " 41: 'key',\n",
       " 42: 'i',\n",
       " 43: 'favorite',\n",
       " 44: 'both',\n",
       " 45: 'be',\n",
       " 46: 'are',\n",
       " 47: 'an',\n",
       " 48: 'would',\n",
       " 49: 'widely',\n",
       " 50: 'when',\n",
       " 51: 'what',\n",
       " 52: 'we',\n",
       " 53: 'way',\n",
       " 54: 'waves',\n",
       " 55: 'voiceactivated',\n",
       " 56: 'voice',\n",
       " 57: 'visualization',\n",
       " 58: 'versatility',\n",
       " 59: 'versatile',\n",
       " 60: 'vehicles',\n",
       " 61: 'use',\n",
       " 62: 'unlocks',\n",
       " 63: 'turning',\n",
       " 64: 'treasure',\n",
       " 65: 'transforming',\n",
       " 66: 'things',\n",
       " 67: 'that',\n",
       " 68: 'thanks',\n",
       " 69: 'telescopes',\n",
       " 70: 'techniques',\n",
       " 71: 'teaching',\n",
       " 72: 'tag',\n",
       " 73: 'systems',\n",
       " 74: 'supply',\n",
       " 75: 'strategies',\n",
       " 76: 'sports',\n",
       " 77: 'simplicity',\n",
       " 78: 'sets',\n",
       " 79: 'service',\n",
       " 80: 'search',\n",
       " 81: 'seaborn',\n",
       " 82: 'scraping',\n",
       " 83: 'scientists',\n",
       " 84: 'risk',\n",
       " 85: 'rich',\n",
       " 86: 'retail',\n",
       " 87: 'responses',\n",
       " 88: 'read',\n",
       " 89: 'raw',\n",
       " 90: 'rapid',\n",
       " 91: 'predict',\n",
       " 92: 'powers',\n",
       " 93: 'power',\n",
       " 94: 'pod',\n",
       " 95: 'piece',\n",
       " 96: 'personalized',\n",
       " 97: 'performance',\n",
       " 98: 'peas',\n",
       " 99: 'paving',\n",
       " 100: 'other',\n",
       " 101: 'optimizing',\n",
       " 102: 'numpy',\n",
       " 103: 'nothing',\n",
       " 104: 'not',\n",
       " 105: 'news',\n",
       " 106: 'meaningful',\n",
       " 107: 'me',\n",
       " 108: 'matplotlib',\n",
       " 109: 'marketing',\n",
       " 110: 'map',\n",
       " 111: 'makes',\n",
       " 112: 'machine',\n",
       " 113: 'linguistics',\n",
       " 114: 'life',\n",
       " 115: 'lies',\n",
       " 116: 'library',\n",
       " 117: 'languages',\n",
       " 118: 'just',\n",
       " 119: 'into',\n",
       " 120: 'insights',\n",
       " 121: 'industry',\n",
       " 122: 'indispensable',\n",
       " 123: 'identify',\n",
       " 124: 'hunt',\n",
       " 125: 'high',\n",
       " 126: 'helping',\n",
       " 127: 'heart',\n",
       " 128: 'goto',\n",
       " 129: 'giving',\n",
       " 130: 'give',\n",
       " 131: 'game',\n",
       " 132: 'future',\n",
       " 133: 'friend',\n",
       " 134: 'frameworks',\n",
       " 135: 'finance',\n",
       " 136: 'fascinating',\n",
       " 137: 'fake',\n",
       " 138: 'experts',\n",
       " 139: 'excellent',\n",
       " 140: 'every',\n",
       " 141: 'engines',\n",
       " 142: 'empathic',\n",
       " 143: 'emotions',\n",
       " 144: 'efficient',\n",
       " 145: 'ecosystem',\n",
       " 146: 'easier',\n",
       " 147: 'ears',\n",
       " 148: 'door',\n",
       " 149: 'django',\n",
       " 150: 'disasters',\n",
       " 151: 'detect',\n",
       " 152: 'decisionmaking',\n",
       " 153: 'day',\n",
       " 154: 'customer',\n",
       " 155: 'comes',\n",
       " 156: 'combine',\n",
       " 157: 'coding',\n",
       " 158: 'chains',\n",
       " 159: 'categorize',\n",
       " 160: 'capabilities',\n",
       " 161: 'cake',\n",
       " 162: 'by',\n",
       " 163: 'businesses',\n",
       " 164: 'breeze',\n",
       " 165: 'best',\n",
       " 166: 'being',\n",
       " 167: 'beginners',\n",
       " 168: 'beats',\n",
       " 169: 'autonomous',\n",
       " 170: 'automatically',\n",
       " 171: 'automated',\n",
       " 172: 'at',\n",
       " 173: 'astronomy',\n",
       " 174: 'assessment',\n",
       " 175: 'articles',\n",
       " 176: 'applied',\n",
       " 177: 'apart',\n",
       " 178: 'amazing',\n",
       " 179: 'allows',\n",
       " 180: 'accuracy'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of vocab\n",
    "vocab = sequence_vectorizer.get_vocabulary()\n",
    "int_to_str = {idx: word for idx, word in enumerate(vocab)}\n",
    "int_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78a8373f-b783-48ff-a42d-463dd3a79bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pod'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What term corresponds to 94?\n",
    "int_to_str[94]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
